@Book{GoluVanl96,
  Title                    = {Matrix Computations},
  Author                   = {Golub, Gene H. and Van Loan, Charles F.},
  Publisher                = {The Johns Hopkins University Press},
  Year                     = {1996},
  Edition                  = {Third}
}

@article{merkel2014docker,
  title={Docker: lightweight linux containers for consistent development and deployment},
  author={Merkel, Dirk},
  journal={Linux journal},
  volume={2014},
  number={239},
  pages={2},
  year={2014}
}

@ARTICLE{30711,  author={},  journal={ANSI/IEEE Std 754-1985},   title={IEEE Standard for Binary Floating-Point Arithmetic},   year={1985},  volume={},  number={},  pages={1-20},  doi={10.1109/IEEESTD.1985.82928}}

@ARTICLE{4610935,  author={},  journal={IEEE Std 754-2008},   title={IEEE Standard for Floating-Point Arithmetic},   year={2008},  volume={},  number={},  pages={1-70},  doi={10.1109/IEEESTD.2008.4610935}}

@article{10.1145/1356052.1356053,
author = {Goto, Kazushige and Geijn, Robert A. van de},
title = {Anatomy of High-Performance Matrix Multiplication},
year = {2008},
issue_date = {May 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {3},
issn = {0098-3500},
url = {https://doi.org/10.1145/1356052.1356053},
doi = {10.1145/1356052.1356053},
abstract = {We present the basic principles that underlie the high-performance implementation
of the matrix-matrix multiplication that is part of the widely used GotoBLAS library.
Design decisions are justified by successively refining a model of architectures with
multilevel memories. A simple but effective algorithm for executing this operation
results. Implementations on a broad selection of architectures are shown to achieve
near-peak performance.},
journal = {ACM Trans. Math. Softw.},
month = may,
articleno = {12},
numpages = {25},
keywords = {matrix multiplication, basic linear algebra subprogrms, Linear algebra}
}

@INPROCEEDINGS{6413635,  author={Xianyi, Zhang and Qian, Wang and Yunquan, Zhang},  booktitle={2012 IEEE 18th International Conference on Parallel and Distributed Systems},   title={Model-driven Level 3 BLAS Performance Optimization on Loongson 3A Processor},   year={2012},  volume={},  number={},  pages={684-691},  doi={10.1109/ICPADS.2012.97}}

@article{SDP,
    author = {Vandenberghe, Lieven and Boyd, Stephen},
    title = {Semidefinite Programming},
    year = {1996},
    issue_date = {Mar. 1996},
    publisher = {Society for Industrial and Applied Mathematics},
    address = {USA},
    volume = {38},
    number = {1},
    issn = {0036-1445},
    url = {https://doi.org/10.1137/1038003},
    doi = {10.1137/1038003},
    journal = {SIAM Rev.},
    month = mar,
    pages = {49–95},
    numpages = {47},
    keywords = {combinatorial optimization, semidefinite programming, system and control theory, eigenvalue optimization, interior-point methods, convex optimization}
}


@INPROCEEDINGS{SDPA-GMP,  author={Nakata, Maho},  booktitle={2010 IEEE International Symposium on Computer-Aided Control System Design},   title={A numerical evaluation of highly accurate multiple-precision arithmetic version of semidefinite programming solver: SDPA-GMP, -QD and -DD.},   year={2010},  volume={},  number={},  pages={29-34},  doi={10.1109/CACSD.2010.5612693}}

@MISC{sdpa-gmpgithub,
    author = "{Nakata, Maho}",
    Title = "{SDPA-GMP}",
    note = {\url{https://github.com/nakatamaho/sdpa-gmp/}},
    year = {retrieved September 23, 2021}
}

@MISC{sdpa-ddgithub,
    author = "{Nakata, Maho}",
    Title = "{SDPA-DD}",
    note = {\url{https://github.com/nakatamaho/sdpa-dd/}},
    year = {retrieved September 23, 2021}
}

@MISC{sdpa-qdgithub,
    author = "{Nakata, Maho}",
    Title = "{SDPA-QD}",
    note = {\url{https://github.com/nakatamaho/sdpa-qd/}},
    year = {retrieved September 23, 2021}
}

@article{JCP2008,
author = {Nakata,Maho  and Braams,Bastiaan J.  and Fujisawa,Katsuki  and Fukuda,Mituhiro  and Percus,Jerome K.  and Yamashita,Makoto  and Zhao,Zhengji },
title = {Variational calculation of second-order reduced density matrices by strong N-representability conditions and an accurate semidefinite programming solver},
journal = {The Journal of Chemical Physics},
volume = {128},
number = {16},
pages = {164113},
year = {2008},
doi = {10.1063/1.2911696},

URL = {
        https://doi.org/10.1063/1.2911696

},
eprint = {
        https://doi.org/10.1063/1.2911696
}
}

@inbook{SDPA,
   author = {Yamashita, Makoto and Fujisawa, Katsuki and Fukuda, Mituhiro and Kobayashi, Kazuhiro and Nakata, Kazuhide and Nakata, Maho},
   editor = {Anjos, Miguel F. and Lasserre, Jean B.},
   title = {Latest Developments in the Family for Solving Large-Scale SDPs},
   booktitle = {Handbook on Semidefinite, Conic and Polynomial Optimization},
   pages = {687-713},
   publisher = {Springer US},
   address = {Boston, MA},
   year = {2012},
   url = {https://doi.org/10.1007/978-1-4614-0769-0_24},
   abstract = {The main purpose of this chapter is to introduce the latest developments in SDPA and its family. SDPA is designed to solve large-scale SemiDefinite Programs (SDPs) faster and over the course of 15 years of development, it has been expanded into a high-performance-oriented software package. We hope that this introduction to the latest developments of the SDPA Family will be beneficial to readers who wish to understand the inside of state-of-art software packages for solving SDPs.}
}

@Article{math3020337,
AUTHOR = {Bailey, David H. and Borwein, Jonathan M.},
TITLE = {High-Precision Arithmetic in Mathematical Physics},
JOURNAL = {Mathematics},
VOLUME = {3},
YEAR = {2015},
NUMBER = {2},
PAGES = {337--367},
URL = {https://www.mdpi.com/2227-7390/3/2/337},
ISSN = {2227-7390},
ABSTRACT = {For many scientific calculations, particularly those involving empirical data, IEEE 32-bit floating-point arithmetic produces results of sufficient accuracy, while for other applications IEEE 64-bit floating-point is more appropriate. But for some very demanding applications, even higher levels of precision are often required. This article discusses the challenge of high-precision computation, in the context of mathematical physics, and highlights what facilities are required to support future computation, in light of emerging developments in computer architecture.},
DOI = {10.3390/math3020337}
}
@article{BAILEY201210106,
title = {High-precision computation: Mathematical physics and dynamics},
journal = {Applied Mathematics and Computation},
volume = {218},
number = {20},
pages = {10106-10121},
year = {2012},
issn = {0096-3003},
doi = {https://doi.org/10.1016/j.amc.2012.03.087},
url = {https://www.sciencedirect.com/science/article/pii/S0096300312003505},
author = {D.H. Bailey and R. Barrio and J.M. Borwein},
keywords = {High-precision computation, Mathematical physics, Dynamical systems, Experimental mathematics},
abstract = {At the present time, IEEE 64-bit floating-point arithmetic is sufficiently accurate for most scientific applications. However, for a rapidly growing body of important scientific computing applications, a higher level of numeric precision is required. Such calculations are facilitated by high-precision software packages that include high-level language translation modules to minimize the conversion effort. This paper presents an overview of recent applications of these techniques and provides some analysis of their numerical requirements. We conclude that high-precision arithmetic facilities are now an indispensable component of a modern large-scale scientific computing environment.}
}

@book{high:ASNA2,
  address =       {Philadelphia, PA, USA},
  author =        {Nicholas J. Higham},
  edition =       {Second},
  pages =         {xxx+680},
  publisher =     {Society for Industrial and Applied Mathematics},
  title =         {Accuracy and Stability of Numerical Algorithms},
  year =          {2002},
  doi =           {10.1137/1.9780898718027},
  isbn =          {0-89871-521-0},
}

@INPROCEEDINGS{IBMz13,
  author={Lichtenau, Cedric and Carlough, Steven and Mueller, Silvia Melitta},
  booktitle={2016 IEEE 23nd Symposium on Computer Arithmetic (ARITH)}, 
  title={Quad Precision Floating Point on the IBM z13}, 
  year={2016},
  volume={},
  number={},
  pages={87-94},
  doi={10.1109/ARITH.2016.26}}

@article{Dekker1971,
author = {Dekker, T.J.},
journal = {Numerische Mathematik},
pages = {224-242},
title = {A Floating-Point Technique for Extending the Available Precision.},
url = {http://eudml.org/doc/132105},
volume = {18},
year = {1971/72},
}

@INPROCEEDINGS{Hida2000,
  author={Yozo Hida, Xiaoye S. Li and David H. Bailey},
  booktitle={Technical Report LBNL-46996}, 
  publisher = {Lawrence Berkley National Laboratory},
  title={Quad-Double Arithmetic: Algorithms, Implementation, and Application}, 
  year={2000},
}

@ARTICLE{8519659,  author={Dongarra, Jack and Getov, Vladimir and Walsh, Kevin},  journal={Computer},   title={The 30th Anniversary of the Supercomputing Conference: Bringing the Future Closer—Supercomputing History and the Immortality of Now},   year={2018},  volume={51},  number={10},  pages={74-85},  doi={10.1109/MC.2018.3971352}}


@book{Knuth1997,
author = {Knuth, Donald E.},
title = {The Art of Computer Programming, Volume 2 (3rd Ed.): Seminumerical Algorithms},
year = {1997},
isbn = {0201896842},
publisher = {Addison-Wesley Longman Publishing Co., Inc.},
address = {USA}
}

@book{18661-3,
title = {ISO/IEC TS 18661-3:2015 Information Technology - Programming languages, their environments, and system software interfaces - Floating-point extensions for C - Part 3: Interchange and extended types},
year = {2015},
publisher = {the International Organization for Standardization},
address = {Chemin de Blandonnet 8 CP 401 1214 Vernier, Geneva Switzerland}
}

@article{fable,
   author = {Grosse-Kunstleve, Ralf W. and Terwilliger, Thomas C. and Sauter, Nicholas K. and Adams, Paul D.},
   title = {Automatic Fortran to++ conversion with},
   journal = {Source Code for Biology and Medicine},
   volume = {7},
   pages = {5},
   year = {2012},
   note = {\url{https://doi.org/10.1186/1751-0473-7-5}},
   abstract = {In scientific computing, Fortran was the dominant implementation language throughout most of the second part of the 20th century. The many tools accumulated during this time have been difficult to integrate with modern software, which is now dominated by object-oriented languages.}
}

@article{10.1145/355841.355847,
author = {Lawson, C. L. and Hanson, R. J. and Kincaid, D. R. and Krogh, F. T.},
title = {Basic Linear Algebra Subprograms for Fortran Usage},
year = {1979},
issue_date = {Sept. 1979},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
issn = {0098-3500},
url = {https://doi.org/10.1145/355841.355847},
doi = {10.1145/355841.355847},
journal = {ACM Trans. Math. Softw.},
month = sep,
pages = {308–323},
numpages = {16}
}

@article{10.1145/42288.42291,
author = {Dongarra, Jack J. and Du Croz, Jeremy and Hammarling, Sven and Hanson, Richard J.},
title = {An Extended Set of FORTRAN Basic Linear Algebra Subprograms},
year = {1988},
issue_date = {March 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {1},
issn = {0098-3500},
url = {https://doi.org/10.1145/42288.42291},
doi = {10.1145/42288.42291},
abstract = {This paper describes an extension to the set of Basic Linear Algebra Subprograms.
The extensions are targeted at matrix-vector operations that should provide for efficient
and portable implementations of algorithms for high-performance computers.},
journal = {ACM Trans. Math. Softw.},
month = mar,
pages = {1–17},
numpages = {17}
}

@article{10.1145/42288.42291_2,
author = {Dongarra, Jack J. and Du Croz, Jeremy and Hammarling, Sven and Hanson, Richard J.},
title = {An Extended Set of FORTRAN Basic Linear Algebra Subprograms},
year = {1988},
issue_date = {March 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {1},
journal = {ACM Trans. Math. Softw.},
month = mar,
pages = {18-32},
numpages = {15}
}

@article{10.1145/567806.567807,
title = {An Updated Set of Basic Linear Algebra Subprograms (BLAS)},
year = {2002},
issue_date = {June 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0098-3500},
url = {https://doi.org/10.1145/567806.567807},
doi = {10.1145/567806.567807},
journal = {ACM Trans. Math. Softw.},
month = jun,
pages = {135–151},
numpages = {17},
keywords = {linear algebra, BLAS, standards}
}

@BOOK{laug,
      AUTHOR = {Anderson, E. and Bai, Z. and Bischof, C. and
                Blackford, S. and Demmel, J. and Dongarra, J. and
                Du Croz, J. and Greenbaum, A. and Hammarling, S. and
                McKenney, A. and Sorensen, D.},
      TITLE = {{LAPACK} Users' Guide},
      EDITION = {Third},
      PUBLISHER = {Society for Industrial and Applied Mathematics},
      YEAR = {1999},
      ADDRESS = {Philadelphia, PA},
      ISBN = {0-89871-447-8 (paperback)} }
@article{tdb10,
    title     = {{Towards dense linear algebra for hybrid GPU accelerated manycore systems}},
    author    = {Stanimire Tomov and Jack Dongarra and Marc Baboulin},
    booktitle = {Parallel Matrix Algorithms and Applications},
    doi       = {10.1016/j.parco.2009.12.005},
    issn      = {0167-8191},
    journal   = {Parallel Computing},
    month     = jun,
    number    = {5-6},
    pages     = {232--240},
    posted-at = {2010-12-17 09:48:58},
    priority  = {2},
    volume    = {36},
    year      = {2010}
}

@misc{cublas,
  author={NVIDIA},
  title={Basic Linear Algebra on NVIDIA GPUs},
  year={2021},
  note={\url{https://developer.nvidia.com/cublas}},
} 

@Manual{Granlund12,
  title = 	 "{GNU MP}: {T}he {GNU} {M}ultiple {P}recision
		  {A}rithmetic {L}ibrary",
  author = "Torbjörn Granlund and {the GMP development team}",
  edition = 	 "5.0.5",
  year = 	 2012,
  note = "\url{http://gmplib.org/}"
}

@article{10.1145/1236463.1236468,
author = {Fousse, Laurent and Hanrot, Guillaume and Lef\`{e}vre, Vincent and P\'{e}lissier, Patrick and Zimmermann, Paul},
title = {MPFR: A Multiple-Precision Binary Floating-Point Library with Correct Rounding},
year = {2007},
issue_date = {June 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {2},
issn = {0098-3500},
url = {https://doi.org/10.1145/1236463.1236468},
doi = {10.1145/1236463.1236468},
abstract = {This article presents a multiple-precision binary floating-point library, written
in the ISO C language, and based on the GNU MP library. Its particularity is to extend
to arbitrary-precision, ideas from the IEEE 754 standard, by providing correct rounding
and exceptions. We demonstrate how these strong semantics are achieved---with no significant
slowdown with respect to other arbitrary-precision tools---and discuss a few applications
where such a library can be useful.},
journal = {ACM Trans. Math. Softw.},
month = jun,
pages = {13–es},
numpages = {15},
keywords = {elementary function, IEEE 754 standard, portable software, correct rounding, floating-point arithmetic, Multiple-precision arithmetic}
}

@manual {mpc,
   author       = {Andreas Enge and Micka\"el Gastineau and
                   Philippe Th\'eveny and Paul Zimmermann},
   title        = {mpc --- A library for multiprecision complex
                   arithmetic with exact rounding},
   organization = {INRIA},
   edition      = {1.1.0},
   year         = {2018},
   month        = jan,
   note         = {\url{http://mpc.multiprecision.org/}}
}

@inproceedings{Gregory1969ACO,
  title={A collection of matrices for testing computational algorithms},
  author={R. T. Gregory and D. Karney},
  year={1969}
}

@TechReport{lawn41,
  author =       "Susan Blackford and Jack Dongarra",
  title =        "Installation Guide for {LAPACK}",
  type =         "LAPACK Working Note",
  number =       "41",
  institution =  inst-UTK-CS,
  address =      inst-UTK-CS:adr,
  month =        mar,
  year =         "1992",
  bibdate =      "Fri Apr 22 17:06:37 2005",
  bibsource =    "http://www.math.utah.edu/pub/tex/bib/lawn.bib",
  note =         "UT-CS-92-151, March, 1992.",
  URL =          "http://www.netlib.org/lapack/lawns/lawn41.ps;
                 http://www.netlib.org/lapack/lawnspdf/lawn41.pdf",
  acknowledgement = ack-nhfb,
}

@INPROCEEDINGS{6424545,  author={Nakata, Maho and Takao, Yasuyoshi and Noda, Shigeho and Himeno, Ryutaro},  booktitle={2012 Third International Conference on Networking and Computing},   title={A Fast Implementation of Matrix-matrix Product in Double-double Precision on NVIDIA C2050 and Application to Semidefinite Programming},   year={2012},  volume={},  number={},  pages={68-75},  doi={10.1109/ICNC.2012.19}}

@INPROCEEDINGS{6495966,

  author={Nakata, Maho},

  booktitle={2012 SC Companion: High Performance Computing, Networking Storage and Analysis}, 

  title={Poster: MPACK 0.7.0: Multiple Precision Version of BLAS and LAPACK}, 

  year={2012},

  volume={},

  number={},

  pages={1353-1353},

  doi={10.1109/SC.Companion.2012.183}}


@InProceedings{10.1007/978-3-030-64616-5_4,
author="Isupov, Konstantin
and Knyazkov, Vladimir",
editor="Voevodin, Vladimir
and Sobolev, Sergey",
title="Multiple-Precision BLAS Library for Graphics Processing Units",
booktitle="Supercomputing",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="37--49",
abstract="The binary32 and binary64 floating-point formats provide good performance on current hardware, but also introduce a rounding error in almost every arithmetic operation. Consequently, the accumulation of rounding errors in large computations can cause accuracy issues. One way to prevent these issues is to use multiple-precision floating-point arithmetic. This paper presents a new library of basic linear algebra operations with multiple precision for graphics processing units. The library is written in CUDA C/C++ and uses the residue number system to represent multiple-precision significands of floating-point numbers. The supported data types, memory layout, and main features of the library are considered. Experimental results are presented showing the performance of the library.",
isbn="978-3-030-64616-5"
}
@article{SAITOH20132005,
title = {ZKCM: A C++ library for multiprecision matrix computation with applications in quantum information},
journal = {Computer Physics Communications},
volume = {184},
number = {8},
pages = {2005-2020},
year = {2013},
issn = {0010-4655},
doi = {https://doi.org/10.1016/j.cpc.2013.03.022},
url = {https://www.sciencedirect.com/science/article/pii/S0010465513001306},
author = {Akira SaiToh},
keywords = {Multiprecision computing, Linear algebra, Time-dependent matrix product state, Quantum information},
abstract = {ZKCM is a C++ library developed for the purpose of multiprecision matrix computation, on the basis of the GNU MP and MPFR libraries. It provides an easy-to-use syntax and convenient functions for matrix manipulations including those often used in numerical simulations in quantum physics. Its extension library, ZKCM_QC, is developed for simulating quantum computing using the time-dependent matrix-product-state simulation method. This paper gives an introduction about the libraries with practical sample programs.
Program Summary
Program title: ZKCM Catalogue identifier: AEPI_v1_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEPI_v1_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: GNU Lesser General Public License, version 3 No. of lines in distributed program, including test data, etc.: 95600 No. of bytes in distributed program, including test data, etc.: 1133481 Distribution format: tar.gz Programming language: C++. Computer: General computers. Operating system: Unix-like systems, such as Linux, Free BSD, Cygwin on Windows OS, etc. RAM: Several mega bytes–several giga bytes, dependent on the problem instance Classification: 4.8, 4.15. External routines: GNU MP (GMP) [1], MPFR [2] Ver. 3.0.0 or later Nature of problem: Multiprecision computation is helpful to guarantee and/or evaluate the accuracy of simulation results in numerical physics. There is a potential demand for a programming library focusing on matrix computation usable for this purpose with a user-friendly syntax. Solution method: A C++ library ZKCM has been developed for multiprecision matrix computation. It provides matrix operations useful for numerical studies of physics, e.g., the tensor product (Kronecker product), the tracing-out operation, the inner product, the LU decomposition, the Hermitian-matrix diagonalization, the singular-value decomposition, and the discrete Fourier transform. For basic floating-point operations, GMP and MPFR libraries are used. An extension library ZKCM QC has also been developed, which employs the time-dependent matrix-product-state method to simulate quantum computing. Restrictions: Multiprecision computation with more than a half thousand bit precision is often a thousand times slower than double-precision computation for any kind of matrix computation. Additional comments: A user’s manual is placed in the directory “doc” of the package. Each function is explained in a reference manual found in the directories “doc/html” and “doc/latex”. Sample programs are placed in the directory “samples”. Running time: It takes less than thirty seconds to obtain a DFT spectrum for 216 data points of a time evolution of a quantum system described by a 4×4 matrix Hamiltonian for 256-bit precision when we use recent AMD or Intel CPU with 2.5 GHz or more CPU frequency. It takes three to five minutes to diagonalize a 100×100 Hermitian matrix for 512-bit precision using the aforementioned CPU. References: [1] The GNU Multiple Precision Arithmetic Library, http://gmplib.org/. [2] L. Fousse et al., MPFR: A multiple-precision binary floating-point library with correct rounding, ACM Trans. Math. Software 33 (2007) 13, http://www.mpfr.org/.}
}

@misc{1611.02831,
Author = {Fredrik Johansson},
Title = {Arb: Efficient Arbitrary-Precision Midpoint-Radius Interval Arithmetic},
Year = {2016},
Eprint = {arXiv:1611.02831},
note={\url{https://arblib.org/}}
}

@MISC{advanpix,
    author = "{Advanpix}",
    Title = "{Multiprecision Computing Toolbox for MATLAB}",
    note = {\url{https://www.advanpix.com/}},
    year = {retrieved September 23, 2021}
}

@MISC{GenericSchur,
    author = "{RalphAS}",
    Title = "{Schur decomposition of matrices with generic floating-point element types in Julia}",
    note = {\url{https://github.com/RalphAS/GenericSchur.jl}},
    year = {retrieved September 23, 2021},
}

@manual{mpmath,
  author  = {Fredrik Johansson and others},
  title   = {mpmath: a {P}ython library for arbitrary-precision floating-point arithmetic (version 0.18)},
  note    = {\url{http://mpmath.org/}},
  month   = {December},
  year    = {2013},
}

@misc{Mathematica,
  author = {Wolfram Research{,} Inc.},
  title = {Mathematica, {V}ersion 12.3.1},
  url = {https://www.wolfram.com/mathematica},
  note = {Champaign, IL, 2021}
}

@InProceedings{10.1007/978-3-642-28151-8_25,
author="Mukunoki, Daichi
and Takahashi, Daisuke",
editor="J{\'o}nasson, Kristj{\'a}n",
title="Implementation and Evaluation of Quadruple Precision BLAS Functions on GPUs",
booktitle="Applied Parallel and Scientific Computing",
year="2012",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="249--259",
abstract="We implemented the quadruple precision Basic Linear Algebra Subprograms (BLAS) functions, AXPY, GEMV and GEMM, on graphics processing units (GPUs), and evaluated their performance. We used DD-type quadruple precision operations, which combine two double precision values to represent a quadruple precision value. On an NVIDIA Tesla C1060, our BLAS functions are up to approximately 30 times faster than the existing quadruple precision BLAS on an Intel Core i7 920. Additionally, the execution time of quadruple precision AXPY takes only approximately 2.7 times longer than that of double precision AXPY on the Tesla C1060. We have shown that quadruple precision BLAS operations are suitable for GPUs.",
isbn="978-3-642-28151-8"
}

@INPROCEEDINGS{7965202,
  author={Yamada, Susumu and Ina, Takuya and Sasa, Narimasa and Idomura, Yasuhiro and Machida, Masahiko and Imamura, Toshiyuki},
  booktitle={2017 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)}, 
  title={Quadruple-precision BLAS using Bailey's arithmetic with FMA instruction: its performance and applications}, 
  year={2017},
  pages={1418-1425},
  doi={10.1109/IPDPSW.2017.42},
}

@INPROCEEDINGS{HPC-137,
  author={Yamada, Susumu and Ina, Takuya and Sasa, Narimasa and Idomura, Yasuhiro and Machida, Masahiko and Imamura, Toshiyuki},
  booktitle={IPSJ SIG Technical Report}, 
  title={Introduction to the quadruple precision basic linear algebra routine group QPBLAS and its applications ({\it in Japanese})}, 
  year={2012},
  volume={137},
  number={23},
  pages={1-6},
}

@manual{qpblas-gpu,
  key     = {QPBLAS-GPU},
  author  = {Japan Atomic Energy Agency},
  title   = {Quadruple Precision BLAS Routines for GPU QPBLAS-GPU Ver.1.0 User’s Manual },
  memo    = {\url{https://ccse.jaea.go.jp/software/QPBLAS-GPU/1.0/manual/qpblas-gpu_manual_en-1.0.pdf}},
  month   = {July},
  year    = {2013},
}

@INPROCEEDINGS{HPC-137_1,
  author={Daichi, Mukunoki and Daisuke, Takahashi},
  booktitle={IPSJ SIG Technical Report}, 
  title={Implementation and Evaluation of Quadruple Precision BLAS on GPU ({\it in Japanese})}, 
  year={2009},
  volume={137},
  number={13},
  pages={1-6},
}

@INPROCEEDINGS{HPC-121,
  author={Nakasato, Naohito and Ishikawa Tadashi and Makino, Junichiro and Yuasa, Fukuko },
  booktitle={IPSJ SIG Technical Report}, 
  title={Quadruple precision arithmetic with accelerators ({\it in Japanese})}, 
  year={2009},
  volume={121},
  number={39},
  pages={1-6},
}

@InProceedings{10.1007/978-3-319-42432-3_29,
author="Joldes, Mioara
and Muller, Jean-Michel
and Popescu, Valentina
and Tucker, Warwick",
editor="Greuel, Gert-Martin
and Koch, Thorsten
and Paule, Peter
and Sommese, Andrew",
title="CAMPARY: Cuda Multiple Precision Arithmetic Library and Applications",
booktitle="Mathematical Software -- ICMS 2016",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="232--240",
abstract="Many scientific computing applications demand massive numerical computations on parallel architectures such as Graphics Processing Units (GPUs). Usually, either floating-point single or double precision arithmetic is used. Higher precision is generally not available in hardware, and software extended precision libraries are much slower and rarely supported on GPUs. We develop CAMPARY: a multiple-precision arithmetic library, using the CUDA programming language for the NVidia GPU platform. In our approach, the precision is extended by representing real numbers as the unevaluated sum of several standard machine precision floating-point numbers. We make use of error-free transforms algorithms, which are based only on native precision operations, but keep track of all rounding errors that occur when performing a sequence of additions and multiplications. This offers the simplicity of using hardware highly optimized floating-point operations, while also allowing for rigorously proven rounding error bounds. This also allows for easy implementation of an interval arithmetic. Currently, all basic multiple-precision arithmetic operations are supported. Our target applications are in chaotic dynamical systems or automatic control.",
isbn="978-3-319-42432-3"
}

@INPROCEEDINGS{8023060,

  author={Joldes, Mioara and Muller, Jean-Michel and Popescu, Valentina},

  booktitle={2017 IEEE 24th Symposium on Computer Arithmetic (ARITH)}, 

  title={Implementation and Performance Evaluation of an Extended Precision Floating-Point Arithmetic Library for High-Accuracy Semidefinite Programming}, 

  year={2017},

  volume={},

  number={},

  pages={27-34},

  doi={10.1109/ARITH.2017.18}}


@article{ISUPOV2020105506,
title = {Performance data of multiple-precision scalar and vector BLAS operations on CPU and GPU},
journal = {Data in Brief},
volume = {30},
pages = {105506},
year = {2020},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2020.105506},
url = {https://www.sciencedirect.com/science/article/pii/S2352340920304005},
author = {Konstantin Isupov},
keywords = {Multiple-precision arithmetic, Floating-point computations, Graphics processing units, CUDA, BLAS},
abstract = {Many optimized linear algebra packages support the single- and double-precision floating-point data types. However, there are a number of important applications that require a higher level of precision, up to hundreds or even thousands of digits. This article presents performance data of four dense basic linear algebra subprograms – ASUM, DOT, SCAL, and AXPY – implemented using existing extended-/multiple-precision software for conventional central processing units and CUDA compatible graphics processing units. The following open source packages are considered: MPFR, MPDECIMAL, ARPREC, MPACK, XBLAS, GARPREC, CAMPARY, CUMP, and MPRES-BLAS. The execution time of CPU and GPU implementations is measured at a fixed problem size and various levels of numeric precision. The data in this article are related to the research article entitled “Design and implementation of multiple-precision BLAS Level 1 functions for graphics processing units” [1].}
}
@manual{MPIBNCpack01,
  author  = {Kouya, Tomonori},
  title   = {MPIBNCpack 0.1},
  note    =  {\url{https://na-inet.jp/na/bnc/mpibncpack.pdf}},
  month   = {September},
  year    = {2003},
}

@manual{BNCpack,
  author  = {Kouya, Tomonori},
  title   = {BNCpack 0.7},
  note    =  {\url{http://na-inet.jp/na/bnc/}},
  month   = {September},
  year    = {2011},
}

@misc{1710.01839,
Author = {Tomonori Kouya},
Title = {Tuning Technique for Multiple Precision Dense Matrix Multiplication using Prediction of Computational Time},
Year = {2017},
Eprint = {arXiv:1710.01839},
}

@InProceedings{10.1007/978-3-030-86976-2_14,
author="Kouya, Tomonori",
editor="Gervasi, Osvaldo
and Murgante, Beniamino
and Misra, Sanjay
and Garau, Chiara
and Ble{\v{c}}i{\'{c}}, Ivan
and Taniar, David
and Apduhan, Bernady O.
and Rocha, Ana Maria A. C.
and Tarantino, Eufemia
and Torre, Carmelo Maria",
title="Acceleration of Multiple Precision Matrix Multiplication Based on Multi-component Floating-Point Arithmetic Using AVX2",
booktitle="Computational Science and Its Applications -- ICCSA 2021",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="202--217",
abstract="In this paper, we report the results obtained from the acceleration of multi-binary64-type multiple precision block and Strassen matrix multiplications with AVX2. We target double-double (DD), triple-double (TD), and quad-double (QD) precision arithmetic designed using certain types of error-free transformation (EFT) arithmetic. Furthermore, we implement SIMDized EFT functions, which simultaneously compute with four binary64 numbers on x86{\_}64 computing environment, and by using help of them, we also develop SIMDized DD, TD, and QD additions and multiplications. In addition, AVX2 load/store functions were adopted to efficiently speed up reading and storing matrix elements from/to memory. Owing to these combined techniques, our implemented multiple precision matrix multiplications were accelerated more than three times compared with non-accelerated ones. Our accelerated matrix multiplication modifies parallelization performance with OpenMP.",
isbn="978-3-030-86976-2"
}

@article{STRASSEN1969,
author = {STRASSEN, V.},
journal = {Numerische Mathematik},
keywords = {numerical analysis},
pages = {354-356},
title = {Gaussian Elimination is not Optimal.},
url = {http://eudml.org/doc/131927},
volume = {13},
year = {1969},
}

@article{Tomonori_Kouya2014,
  title={Accelerated multiple precision matrix multiplication using Strassen's algorithm and Winograd's variant},
  author={Tomonori Kouya},
  journal={JSIAM Letters},
  volume={6},
  number={ },
  pages={81-84},
  year={2014},
  doi={10.14495/jsiaml.6.81}
}

@article{Tomonori_Kouya2016,
  title={Performance evaluation of multiple precision matrix multiplications using parallelized Strassen and Winograd algorithms},
  author={Tomonori Kouya},
  journal={JSIAM Letters},
  volume={8},
  number={ },
  pages={21-24},
  year={2016},
  doi={10.14495/jsiaml.8.21}
}
@inproceedings{Nakayama2011ImplementationOM,
  author={Takato Nakayama and D. Takahashi},
  booktitle={Proc. 23rd IASTED International Conference on Parallel and Distributed Computing and
   Systems (PDCS 2011)}, 
  title={Implementation of multiple-precision floating-point arithmetic library for GPU computing},
  year={2011},
  pages={343-349},
  }
  
@inproceedings{Lu:2010:SEP:1869389.1869392,
 author = {Lu, Mian and He, Bingsheng and Luo, Qiong},
 title = {Supporting Extended Precision on Graphics Processors},
 booktitle = {Proceedings of the Sixth International Workshop on Data Management on New Hardware},
 series = {DaMoN '10},
 year = {2010},
 isbn = {978-1-4503-0189-3},
 location = {Indianapolis, Indiana},
 pages = {19--26},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1869389.1869392},
 doi = {10.1145/1869389.1869392},
 acmid = {1869392},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@Manual{matlabsymbolic,
    title = {Symbolic Math Toolbox},
    author = {The MathWorks, Inc.},
    address = {Natick, Massachusetts, United State},
    year = {2019},
    url = {https://www.mathworks.com/help/symbolic/},
  } 
  
  @MISC{wikipedia_svd,
    Title = "{Singular value decomposition}",
    note = {\url{https://en.wikipedia.org/wiki/Singular_value_decomposition}},
    year = {retrieved September 24, 2021}
}
@inproceedings{hishinuma2019pzqd,
	title={pzqd: PEZY-SC2 Acceleration of Double-Double Precision Arithmetic Library for High-Precision BLAS},
	author={Hishinuma, Toshiaki and Nakata, Maho},
	booktitle={International Conference on Computational \& Experimental Engineering and Sciences},
	pages={717--736},
	year={2019},
	organization={Springer}
}

@article{Nakasato2011AFG,
  title={A fast GEMM implementation on the cypress GPU},
  author={N. Nakasato},
  journal={SIGMETRICS Perform. Evaluation Rev.},
  year={2011},
  volume={38},
  pages={50-55}
}